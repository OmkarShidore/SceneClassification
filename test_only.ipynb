{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models,transforms,datasets\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\Developer\\Machine Learning and Deep Learning\\Pytorch_Training\\Projects\\Building\\datasets\\data\\test\\\n",
      "E:\\Developer\\Machine Learning and Deep Learning\\Pytorch_Training\\Projects\\Building\\datasets\\data\\train\\\n"
     ]
    }
   ],
   "source": [
    "train_data_location=os.path.join(os.getcwd(),'datasets\\\\data\\\\train\\\\')\n",
    "test_data_location=os.path.join(os.getcwd(),'datasets\\\\data\\\\test\\\\')\n",
    "print(test_data_location)\n",
    "print(train_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes=['Building','Forest','Glacier','Mountain','Sea','Street']\n",
    "len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_location='dataset/data'\n",
    "batch_size=32\n",
    "num_workers=1\n",
    "valid_size=0.2\n",
    "\n",
    "train_transform=transforms.Compose([transforms.RandomRotation(30),\n",
    "                                        transforms.RandomResizedCrop(150),\n",
    "                                        transforms.RandomHorizontalFlip(),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                        ])\n",
    "test_transform=transforms.Compose([transforms.Resize(150),\n",
    "                                    transforms.CenterCrop(150),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                    ])\n",
    "\n",
    "train_data=datasets.ImageFolder(data_location+'/train',transform=train_transform)\n",
    "test_data=datasets.ImageFolder(data_location+'/test',transform=test_transform)\n",
    "\n",
    "num_train=len(train_data)\n",
    "indices=list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split=int(np.floor(valid_size*num_train))\n",
    "train_idx,valid_idx=indices[split:],indices[:split]\n",
    "\n",
    "train_sampler=SubsetRandomSampler(train_idx)\n",
    "valid_sampler=SubsetRandomSampler(valid_idx)\n",
    "\n",
    "trainloader=torch.utils.data.DataLoader(train_data,batch_size=batch_size,sampler=train_sampler,num_workers=num_workers)\n",
    "validloader=torch.utils.data.DataLoader(train_data,batch_size=batch_size,sampler=valid_sampler,num_workers=num_workers)\n",
    "testloader=torch.utils.data.DataLoader(test_data,batch_size=batch_size,num_workers=num_workers)\n",
    "\n",
    "classes=['Building','Forest','Glacier','Mountain','Sea','Street']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 14034\n",
       "    Root location: dataset/data/train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               RandomRotation(degrees=(-30, 30), resample=False, expand=False)\n",
       "               RandomResizedCrop(size=(150, 150), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=PIL.Image.BILINEAR)\n",
       "               RandomHorizontalFlip(p=0.5)\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
       "           )"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Found!, Loading Model\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes=['Building','Forest','Glacier','Mountain','Sea','Street']\n",
    "len_classes=len(classes)\n",
    "out_fetures=len_classes\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#importing AlexNet pretrained\n",
    "model=models.alexnet(pretrained=True)\n",
    "#AlexNet was orignally trained for 1000 class labels, we only have 6\n",
    "#replace final layer with a new one\n",
    "model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features,out_fetures)\n",
    "#Loss Funtion and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "try:\n",
    "    checkpoint=torch.load('model.pt')\n",
    "    print('Model Found!, Loading Model')\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    epoch=checkpoint['epoch']\n",
    "    train_loss=checkpoint['train_loss']\n",
    "    valid_loss=checkpoint['valid_loss']\n",
    "    print('Done!')\n",
    "\n",
    "except:\n",
    "    print('Model Not Found, Starting to train from the begining.')\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.286845\n",
      "\n",
      "Test Accuracy of     0: 90% (397/437)\n",
      "Test Accuracy of     1: 99% (472/474)\n",
      "Test Accuracy of     2: 80% (444/553)\n",
      "Test Accuracy of     3: 87% (460/525)\n",
      "Test Accuracy of     4: 94% (481/510)\n",
      "\n",
      "Test Accuracy (Overall): 90% (2707/3000)\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "model.eval() # prep model for evaluation\n",
    "\n",
    "for data, target in testloader:\n",
    "    data,target=data.to(device),target.to(device)\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # update test loss \n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)\n",
    "    # compare predictions to true label\n",
    "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(len(target)):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# calculate and print avg test loss\n",
    "test_loss = test_loss/len(testloader.sampler)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(5):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            str(i), 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imsize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-dbda23ae3ae8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cuda'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m image_loader=transforms.Compose([transforms.Scale(imsize),\n\u001b[0m\u001b[0;32m      3\u001b[0m                           transforms.ToTensor()])\n\u001b[0;32m      4\u001b[0m \u001b[0mtest_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'dataset/Predset/seg_pred/seg_pred/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'imsize' is not defined"
     ]
    }
   ],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "image_loader=transforms.Compose([transforms.Scale(imsize),\n",
    "                          transforms.ToTensor()])\n",
    "test_location='dataset/Predset/seg_pred/seg_pred/'\n",
    "\n",
    "\n",
    "try:\n",
    "    checkpoint=torch.load('model.pt')\n",
    "    print('Model Found!, Loading Model')\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    epoch=checkpoint['epoch']\n",
    "    train_loss=checkpoint['train_loss']\n",
    "    valid_loss=checkpoint['valid_loss']\n",
    "    print('Done!')\n",
    "except:\n",
    "    print('Model Not Found')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
